#!/bin/bash -e

usage() {
        echo "Usage:"
        echo "    $0 libvirt %number_of_kubermesh_nodes%"
        echo "    $0 usb"
}

if [ "$1" == "libvirt" ]; then
  MODE=libvirt
  if ! [[ $2 =~ ^[0-9]+$ ]]; then
    echo "'$2' is not a number"
    usage
    exit 1
  fi
  NUM_NODES=$2
elif [ "$1" == "usb" ]; then
  MODE=usb
  NUM_NODES=0
else
  usage
  exit 1
fi



LIBVIRT_PATH=`pwd`/libvirt
ASSET_DIR=`pwd`/cluster
BOOTCFG_CONFIG_DIR=`pwd`/bootcfg-config
NETWORK_DIR=`pwd`/networks
VM_CONFIGS=`pwd`/machines
USER_DATA_TEMPLATE=${VM_CONFIGS}/gateway/user-data
GATEWAY_IP=2001:db8:a001::2
CHANNEL=alpha
RELEASE=1262.0.0
RAM=1024
CPUs=1
IMG_NAME="coreos_${CHANNEL}_${RELEASE}_qemu_image.img"

if [ ! -d $LIBVIRT_PATH ]; then
        mkdir -p $LIBVIRT_PATH || (echo "Can not create $LIBVIRT_PATH directory" && exit 1)
fi

if [ ! -f $USER_DATA_TEMPLATE ]; then
        echo "$USER_DATA_TEMPLATE template doesn't exist"
        exit 1
fi

echo "Creating networks..."
for network in `ls -1 ${NETWORK_DIR}`; do
  if ! virsh net-info ${network} >/dev/null 2>/dev/null; then
    virsh -q net-define --file ${NETWORK_DIR}/${network}
    virsh -q net-autostart ${network}
    virsh -q net-start ${network}
  fi
done

echo "Generating kubernetes assets..."
rm -rf ${ASSET_DIR}
docker run --name kubermesh-bootkube quay.io/coreos/bootkube:v0.1.2 /bootkube render --asset-dir=cluster --api-servers=https://[fd65:7b9c:569:680:98eb:c508:eb8c:1b80]:443 --etcd-servers=http://[fd65:7b9c:569:680:98eb:c508:ea6b:b0b2]:2379
docker cp kubermesh-bootkube:cluster ${ASSET_DIR}
docker rm kubermesh-bootkube

echo "Copying manifests..."
cp -r manifests/* cluster/manifests/
rm cluster/manifests/kubelet.yaml
rm cluster/manifests/kube-dns-rc.yaml

echo "Generating bootcfg manifests..."
rm -rf ${ASSET_DIR}/bootcfg-config
cp -r ${BOOTCFG_CONFIG_DIR} ${ASSET_DIR}/bootcfg-config
sed -i 's#PASSWORD_HASH#iGyYPV5k4DAzA#' ${ASSET_DIR}/bootcfg-config/groups/*
SSH_PUBLIC_KEY=`cat ~/.ssh/id_rsa.pub`
sed -i "s#SSH_PUBLIC_KEY#${SSH_PUBLIC_KEY}#" ${ASSET_DIR}/bootcfg-config/groups/*
kubectl create configmap --dry-run --namespace kubermesh --output yaml --from-file ${ASSET_DIR}/bootcfg-config/groups bootcfg-groups >cluster/manifests/bootcfg-groups.yaml
kubectl create configmap --dry-run --namespace kubermesh --output yaml --from-file ${ASSET_DIR}/bootcfg-config/ignition bootcfg-ignition >cluster/manifests/bootcfg-ignition.yaml
kubectl create configmap --dry-run --namespace kubermesh --output yaml --from-file ${ASSET_DIR}/bootcfg-config/profiles bootcfg-profiles >cluster/manifests/bootcfg-profiles.yaml

if [ ! -f $LIBVIRT_PATH/$IMG_NAME ]; then
        wget https://${CHANNEL}.release.core-os.net/amd64-usr/${RELEASE}/coreos_production_qemu_image.img.bz2 -O - | bzcat > $LIBVIRT_PATH/$IMG_NAME || (rm -f $LIBVIRT_PATH/$IMG_NAME && echo "Failed to download image" && exit 1)
fi

# Seed node configuration

HOSTNAME="kubermesh-gateway"
echo "Configuring seed node ${HOSTNAME}..."

if [ ! -d $LIBVIRT_PATH/$HOSTNAME/openstack/latest ]; then
        mkdir -p $LIBVIRT_PATH/$HOSTNAME/openstack/latest || (echo "Can not create $LIBVIRT_PATH/$HOSTNAME/openstack/latest directory" && exit 1)
fi

if [ ! -f $LIBVIRT_PATH/$HOSTNAME.qcow2 ]; then
        qemu-img create -q -f qcow2 -b $LIBVIRT_PATH/$IMG_NAME $LIBVIRT_PATH/$HOSTNAME.qcow2
fi

cp $USER_DATA_TEMPLATE $LIBVIRT_PATH/$HOSTNAME/openstack/latest/user_data
sed 's/^/      /' ${ASSET_DIR}/auth/kubeconfig >> $LIBVIRT_PATH/$HOSTNAME/openstack/latest/user_data
sed -i "s#SSH_PUBLIC_KEY#${SSH_PUBLIC_KEY}#" $LIBVIRT_PATH/$HOSTNAME/openstack/latest/user_data
sed -i 's#PASSWORD_HASH#iGyYPV5k4DAzA#' $LIBVIRT_PATH/$HOSTNAME/openstack/latest/user_data

if [ $MODE == "libvirt" ]; then
  NODE_NETWORK="--network network=kubermesh-gateway-1"
elif [ $MODE == "usb" ]; then
  NODE_NETWORK="--hostdev 00:1a.0"
fi

virt-install -q --connect qemu:///system \
             --import \
             --name ${HOSTNAME} \
             --ram 1024 \
             --vcpus 1 \
             --os-type=linux \
             --os-variant=virtio26 \
             --serial pty \
             --disk path=${LIBVIRT_PATH}/${HOSTNAME}.qcow2,format=qcow2,bus=virtio \
             --filesystem ${LIBVIRT_PATH}/${HOSTNAME}/,config-2,type=mount,mode=squash \
             --network network=kubermesh-internet \
             $NODE_NETWORK \
             --vnc \
             --noautoconsole

# Blank node configuration

for SEQ in $(seq 1 $NUM_NODES); do
        HOSTNAME="kubermesh$SEQ"
        echo "Configuring blank node ${HOSTNAME}..."

        if [ ! -f $LIBVIRT_PATH/$HOSTNAME.qcow2 ]; then
                qemu-img create -q -f qcow2 $LIBVIRT_PATH/$HOSTNAME.qcow2 8.5G
        fi

        virt-install -q --connect qemu:///system \
                     --name $HOSTNAME \
                     --ram $RAM \
                     --vcpus $CPUs \
                     --os-type=linux \
                     --os-variant=virtio26 \
                     --serial pty \
                     --disk path=$LIBVIRT_PATH/$HOSTNAME.qcow2,format=qcow2,bus=virtio \
                     `cat ${VM_CONFIGS}/${HOSTNAME}/virt-args` \
                     --vnc \
                     --boot hd,network \
                     --print-xml > ${LIBVIRT_PATH}/${HOSTNAME}.xml
        sed -i -e 's#</os>#<bios useserial="yes" rebootTimeout="10000"/></os>#' ${LIBVIRT_PATH}/${HOSTNAME}.xml
        virsh -q define ${LIBVIRT_PATH}/${HOSTNAME}.xml
        virsh -q start ${HOSTNAME}
done

echo -n 'Waiting for gateway'
while ! ./ssh.sh true >/dev/null 2>/dev/null; do
  sleep 1
  echo -n .
done
echo
echo "Gateway available at: $GATEWAY_IP (or use ./ssh.sh)"

echo "Copying bootcfg config to gateway..."
scp -q -o stricthostkeychecking=no -r ${ASSET_DIR}/bootcfg-config core@[${GATEWAY_IP}]:/home/core/

echo -n 'Waiting for etcd'
while ! ./ssh.sh -q '/usr/bin/etcdctl ls' >/dev/null 2>/dev/null; do
  sleep 1
  echo -n .
done
echo
echo 'Setting flannel network config...'
./ssh.sh -q "/usr/bin/etcdctl --timeout 5s set /coreos.com/network/config '{ \"Network\": \"172.31.0.0/16\", \"Backend\": {\"Type\": \"alloc\"} }' >/dev/null"

echo 'Setting ip-allocator config...'
./ssh.sh -q '/usr/bin/etcdctl --timeout 5s set /kubermesh.github.io/ip-allocator/config/ipv4-base-network 172.30.2.0/24 >/dev/null'
./ssh.sh -q '/usr/bin/etcdctl --timeout 5s set /kubermesh.github.io/ip-allocator/config/ipv6-base-network 2001:db8::/71 >/dev/null'

echo 'Forcing initial anycast addresses up...'
./ssh.sh -q 'sudo ip addr add fd65:7b9c:569:680:98eb:c508:eb8c:1b80 dev cluster'


echo -n 'Waiting for dnsmasq container'
while ! ./ssh.sh -q docker inspect dnsmasq >/dev/null 2>/dev/null; do
  sleep 1
  echo -n .
done
echo

echo -n 'Waiting for bootcfg container'
while ! ./ssh.sh -q docker inspect bootcfg >/dev/null 2>/dev/null; do
  sleep 1
  echo -n .
done
echo

echo -n 'Waiting for kubermesh1 to be available'
while ! ./ssh.sh -q 'ssh -q -o UserKnownHostsFile=/dev/null -o stricthostkeychecking=no `ip neigh show dev cluster | grep 2001 | grep REACHABLE | awk "{print \\$1}"` true'; do
  sleep 1
  echo -n .
done
echo

echo -n 'Waiting for kubermesh1 kubelet'
while ! ./ssh.sh -q 'ssh -q -o UserKnownHostsFile=/dev/null -o stricthostkeychecking=no `ip neigh show dev cluster | grep 2001 | grep REACHABLE | awk "{print \\$1}"` rkt list | grep -q kube'; do
  sleep 1
  echo -n .
done
echo

echo 'Starting bootkube...'
scp -q -o stricthostkeychecking=no -r ${ASSET_DIR} core@[${GATEWAY_IP}]:/home/core/cluster
./ssh.sh -q 'docker run --detach --net=host -v /home/core/cluster:/home/core/cluster --name kubermesh-bootkube quay.io/coreos/bootkube:v0.1.2 /bootkube start --asset-dir=/home/core/cluster --etcd-server=http://127.0.0.1:2379'

echo -n 'Waiting for anycast address to be available on kubermesh1'
while ! ./ssh.sh -q 'docker exec ospf6 vtysh -c "show ipv6 ospf6 database intra-prefix" | grep 172.30.2. | grep -q fd65:7b9c:569:680:98eb:c508:eb8c:1b80'; do
  sleep 1
  echo -n .
done
echo


echo 'Removing anycast address'
./ssh.sh -q 'sudo ip addr del fd65:7b9c:569:680:98eb:c508:eb8c:1b80 dev cluster'

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=${PWD}/cluster/auth/kubeconfig get nodes"
echo "(Alternatively, ./kubectl has been provided for your convenience)"
echo
